{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a883de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== notebooks/01_exploration.ipynb =====\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üß™ Stroke Report NLP Exploration\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook explores the Classical NLP pipeline for extracting keywords from German stroke radiology reports.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Setup and Imports\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('..')  # Add parent directory to import our modules\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from collections import Counter\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Our custom modules\\n\",\n",
    "    \"from extractor.preprocessing import TextPreprocessor\\n\",\n",
    "    \"from extractor.keyword_rules import KeywordExtractor\\n\",\n",
    "    \"from extractor.spacy_ner_wrapper import SpacyNERExtractor\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure plotting\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Initialize NLP Components\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize our NLP pipeline components\\n\",\n",
    "    \"preprocessor = TextPreprocessor()\\n\",\n",
    "    \"keyword_extractor = KeywordExtractor()\\n\",\n",
    "    \"ner_extractor = SpacyNERExtractor()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ NLP components initialized successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Sample Data Creation and Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create extended sample reports for analysis\\n\",\n",
    "    \"sample_reports = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'id': 'report_001',\\n\",\n",
    "    \"        'text': \\\"\\\"\\\"\\n\",\n",
    "    \"        Patient wurde in Allgemeinan√§sthesie behandelt. Beginn der Intervention um 08:32 Uhr.\\n\",\n",
    "    \"        Verwendung des Trevo Stentretriever Systems. rtPA wurde um 07:45 verabreicht.\\n\",\n",
    "    \"        Mechanische Thrombektomie mit SOFIA Katheter durchgef√ºhrt.\\n\",\n",
    "    \"        Finales Ergebnis: TICI 3. Keine Komplikationen aufgetreten.\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'id': 'report_002', \\n\",\n",
    "    \"        'text': \\\"\\\"\\\"\\n\",\n",
    "    \"        Sedierung f√ºr die Prozedur. Start: 09:15 Uhr mit Aspiration using Penumbra System.\\n\",\n",
    "    \"        Catch Mini device eingesetzt. Urokinase als Thrombolytikum verwendet.\\n\",\n",
    "    \"        Leichte Blutung nach der Intervention beobachtet. TICI 2b erreicht.\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'id': 'report_003',\\n\",\n",
    "    \"        'text': \\\"\\\"\\\"\\n\",\n",
    "    \"        Lokale An√§sthesie f√ºr den Eingriff. Beginn: 10:20. \\n\",\n",
    "    \"        Solitaire Stentretriever verwendet f√ºr die mechanische Rekanalisation.\\n\",\n",
    "    \"        Heparin antikoagulation. Embotrap als backup device.\\n\",\n",
    "    \"        Perforation der Gef√§√üwand aufgetreten. TICI 1 Ergebnis.\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'id': 'report_004',\\n\",\n",
    "    \"        'text': \\\"\\\"\\\"\\n\",\n",
    "    \"        Vollnarkose eingeleitet um 11:45 Uhr. Tenecteplase i.v. verabreicht.\\n\",\n",
    "    \"        Mechanische Thrombektomie mit Trevo Stentretriever und SOFIA Aspiration.\\n\",\n",
    "    \"        Zus√§tzlich Aspirin und Heparin gegeben. TICI 2a erreicht.\\n\",\n",
    "    \"        Postinterventionelles H√§matom festgestellt.\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'id': 'report_005',\\n\",\n",
    "    \"        'text': \\\"\\\"\\\"\\n\",\n",
    "    \"        Patient unter Sedierung. Interventionsbeginn: 14:30.\\n\",\n",
    "    \"        Prim√§re Aspiration mit Penumbra System erfolgreich.\\n\",\n",
    "    \"        Keine Medikation erforderlich. Komplikationsloser Verlauf.\\n\",\n",
    "    \"        Finales TICI 3 Ergebnis nach mechanischer Embolektomie.\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Created {len(sample_reports)} sample reports for analysis\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Text Preprocessing Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze preprocessing effects\\n\",\n",
    "    \"print(\\\"=== PREPROCESSING ANALYSIS ===\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, report in enumerate(sample_reports[:2]):  # Show first 2 reports\\n\",\n",
    "    \"    original = report['text']\\n\",\n",
    "    \"    cleaned = preprocessor.clean_text(original)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"üìÑ Report {report['id']}:\\\")\\n\",\n",
    "    \"    print(f\\\"Original length: {len(original)} chars\\\")\\n\",\n",
    "    \"    print(f\\\"Cleaned length: {len(cleaned)} chars\\\")\\n\",\n",
    "    \"    print(f\\\"Original: {original[:100]}...\\\")\\n\",\n",
    "    \"    print(f\\\"Cleaned: {cleaned[:100]}...\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 50)\\n\",\n",
    "    \"    print()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Keyword Extraction Pattern Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test keyword extraction on all samples\\n\",\n",
    "    \"extraction_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for report in sample_reports:\\n\",\n",
    "    \"    cleaned_text = preprocessor.clean_text(report['text'])\\n\",\n",
    "    \"    results = keyword_extractor.extract_all(cleaned_text, report['id'])\\n\",\n",
    "    \"    extraction_results.append(results)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert to DataFrame for analysis\\n\",\n",
    "    \"results_df = pd.DataFrame(extraction_results)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== KEYWORD EXTRACTION RESULTS ===\\\")\\n\",\n",
    "    \"print(results_df.to_string())\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show extraction statistics\\n\",\n",
    "    \"print(\\\"=== EXTRACTION STATISTICS ===\\\")\\n\",\n",
    "    \"categories = ['anesthesia', 'medication', 'treatment_method', 'device', 'tici_score', 'complications']\\n\",\n",
    "    \"\\n\",\n",
    "    \"for category in categories:\\n\",\n",
    "    \"    non_null_count = results_df[category].notna().sum()\\n\",\n",
    "    \"    coverage = (non_null_count / len(results_df)) * 100\\n\",\n",
    "    \"    print(f\\\"{category.replace('_', ' ').title()}: {non_null_count}/{len(results_df)} reports ({coverage:.1f}%)\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Pattern Matching Deep Dive\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze individual pattern performance\\n\",\n",
    "    \"print(\\\"=== PATTERN MATCHING ANALYSIS ===\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test each category's patterns\\n\",\n",
    "    \"test_text = \\\" \\\".join([report['text'] for report in sample_reports])\\n\",\n",
    "    \"cleaned_test_text = preprocessor.clean_text(test_text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for category, patterns in keyword_extractor.patterns.items():\\n\",\n",
    "    \"    print(f\\\"üîç {category.upper()}:\\\")\\n\",\n",
    "    \"    total_matches = 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for pattern in patterns:\\n\",\n",
    "    \"        matches = re.findall(pattern, cleaned_test_text, re.IGNORECASE)\\n\",\n",
    "    \"        if matches:\\n\",\n",
    "    \"            print(f\\\"  Pattern '{pattern}' ‚Üí {matches}\\\")\\n\",\n",
    "    \"            total_matches += len(matches)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"  Pattern '{pattern}' ‚Üí No matches\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  Total matches: {total_matches}\\\")\\n\",\n",
    "    \"    print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test time patterns separately\\n\",\n",
    "    \"print(\\\"‚è∞ TIME PATTERNS:\\\")\\n\",\n",
    "    \"for pattern in keyword_extractor.time_patterns:\\n\",\n",
    "    \"    matches = re.findall(pattern, cleaned_test_text, re.IGNORECASE)\\n\",\n",
    "    \"    if matches:\\n\",\n",
    "    \"        print(f\\\"  Pattern '{pattern}' ‚Üí {matches}\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"  Pattern '{pattern}' ‚Üí No matches\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. spaCy NER Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze spaCy NER performance\\n\",\n",
    "    \"print(\\\"=== spaCy NER ANALYSIS ===\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"all_entities = []\\n\",\n",
    "    \"all_noun_phrases = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for report in sample_reports:\\n\",\n",
    "    \"    cleaned_text = preprocessor.clean_text(report['text'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract entities\\n\",\n",
    "    \"    entities = ner_extractor.extract_entities(cleaned_text)\\n\",\n",
    "    \"    noun_phrases = ner_extractor.extract_noun_phrases(cleaned_text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    all_entities.extend(entities)\\n\",\n",
    "    \"    all_noun_phrases.extend(noun_phrases)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"üìÑ {report['id']}:\\\")\\n\",\n",
    "    \"    print(f\\\"  Entities: {[ent['text'] for ent in entities]}\\\")\\n\",\n",
    "    \"    print(f\\\"  Noun phrases: {noun_phrases[:5]}...\\\")  # Show first 5\\n\",\n",
    "    \"    print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Entity statistics\\n\",\n",
    "    \"if all_entities:\\n\",\n",
    "    \"    entity_labels = [ent['label'] for ent in all_entities]\\n\",\n",
    "    \"    entity_counts = Counter(entity_labels)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Entity Label Distribution:\\\")\\n\",\n",
    "    \"    for label, count in entity_counts.most_common():\\n\",\n",
    "    \"        print(f\\\"  {label}: {count}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"No entities found by spaCy NER\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Visualization and Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create visualizations\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n\",\n",
    "    \"fig.suptitle('Stroke Report NLP Analysis', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Extraction coverage by category\\n\",\n",
    "    \"ax1 = axes[0, 0]\\n\",\n",
    "    \"categories = ['anesthesia', 'medication', 'treatment_method', 'device', 'tici_score', 'complications']\\n\",\n",
    "    \"coverages = [(results_df[cat].notna().sum() / len(results_df)) * 100 for cat in categories]\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars = ax1.bar(range(len(categories)), coverages, color=sns.color_palette(\\\"husl\\\", len(categories)))\\n\",\n",
    "    \"ax1.set_title('Extraction Coverage by Category')\\n\",\n",
    "    \"ax1.set_ylabel('Coverage (%)')\\n\",\n",
    "    \"ax1.set_xticks(range(len(categories)))\\n\",\n",
    "    \"ax1.set_xticklabels([cat.replace('_', '\\\\n') for cat in categories], rotation=45, ha='right')\\n\",\n",
    "    \"ax1.set_ylim(0, 100)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels on bars\\n\",\n",
    "    \"for bar, coverage in zip(bars, coverages):\\n\",\n",
    "    \"    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \\n\",\n",
    "    \"             f'{coverage:.0f}%', ha='center', va='bottom')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 2. Text length distribution\\n\",\n",
    "    \"ax2 = axes[0, 1]\\n\",\n",
    "    \"text_lengths = [len(report['text']) for report in sample_reports]\\n\",\n",
    "    \"ax2.hist(text_lengths, bins=5, color='lightblue', edgecolor='black', alpha=0.7)\\n\",\n",
    "    \"ax2.set_title('Report Text Length Distribution')\\n\",\n",
    "    \"ax2.set_xlabel('Character Count')\\n\",\n",
    "    \"ax2.set_ylabel('Frequency')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 3. Extracted items count\\n\",\n",
    "    \"ax3 = axes[1, 0]\\n\",\n",
    "    \"items_per_report = []\\n\",\n",
    "    \"for _, row in results_df.iterrows():\\n\",\n",
    "    \"    count = sum(1 for cat in categories if pd.notna(row[cat]))\\n\",\n",
    "    \"    items_per_report.append(count)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax3.bar(range(1, len(items_per_report) + 1), items_per_report, \\n\",\n",
    "    \"        color='lightgreen', edgecolor='black')\\n\",\n",
    "    \"ax3.set_title('Extracted Items per Report')\\n\",\n",
    "    \"ax3.set_xlabel('Report Number')\\n\",\n",
    "    \"ax3.set_ylabel('Number of Extracted Items')\\n\",\n",
    "    \"ax3.set_xticks(range(1, len(items_per_report) + 1))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 4. TICI score distribution\\n\",\n",
    "    \"ax4 = axes[1, 1]\\n\",\n",
    "    \"tici_scores = results_df['tici_score'].dropna().tolist()\\n\",\n",
    "    \"if tici_scores:\\n\",\n",
    "    \"    # Handle both single values and lists\\n\",\n",
    "    \"    flat_tici = []\\n\",\n",
    "    \"    for score in tici_scores:\\n\",\n",
    "    \"        if isinstance(score, list):\\n\",\n",
    "    \"            flat_tici.extend(score)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            flat_tici.append(score)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tici_counts = Counter(flat_tici)\\n\",\n",
    "    \"    scores = list(tici_counts.keys())\\n\",\n",
    "    \"    counts = list(tici_counts.values())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    ax4.pie(counts, labels=scores, autopct='%1.1f%%', startangle=90)\\n\",\n",
    "    \"    ax4.set_title('TICI Score Distribution')\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    ax4.text(0.5, 0.5, 'No TICI scores\\\\nfound', ha='center', va='center', \\n\",\n",
    "    \"             transform=ax4.transAxes, fontsize=12)\\n\",\n",
    "    \"    ax4.set_title('TICI Score Distribution')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Pattern Refinement and Testing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test pattern variations and improvements\\n\",\n",
    "    \"print(\\\"=== PATTERN REFINEMENT TESTING ===\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test different anesthesia pattern variations\\n\",\n",
    "    \"test_sentences = [\\n\",\n",
    "    \"    \\\"Patient in Allgemeinan√§sthesie\\\",\\n\",\n",
    "    \"    \\\"Unter Vollnarkose durchgef√ºhrt\\\",\\n\",\n",
    "    \"    \\\"Lokale Bet√§ubung verwendet\\\",\\n\",\n",
    "    \"    \\\"Sedierung w√§hrend der Prozedur\\\",\\n\",\n",
    "    \"    \\\"ITN eingeleitet\\\"  # Intubationsnarkose\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Current patterns\\n\",\n",
    "    \"current_anesthesia_patterns = keyword_extractor.patterns['anesthesia']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Enhanced patterns\\n\",\n",
    "    \"enhanced_anesthesia_patterns = [\\n\",\n",
    "    \"    r'\\\\b(allgemein)?an√§sthesie\\\\b',\\n\",\n",
    "    \"    r'\\\\b(voll)?narkose\\\\b',\\n\",\n",
    "    \"    r'\\\\blokal(e|an√§sthesie)\\\\b',\\n\",\n",
    "    \"    r'\\\\bsedierung\\\\b',\\n\",\n",
    "    \"    r'\\\\bbet√§ubung\\\\b',\\n\",\n",
    "    \"    r'\\\\bitn\\\\b',  # Intubationsnarkose\\n\",\n",
    "    \"    r'\\\\bintubation\\\\b'\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Testing anesthesia pattern variations:\\\")\\n\",\n",
    "    \"for sentence in test_sentences:\\n\",\n",
    "    \"    print(f\\\"\\\\nSentence: '{sentence}'\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Test current patterns\\n\",\n",
    "    \"    current_matches = []\\n\",\n",
    "    \"    for pattern in current_anesthesia_patterns:\\n\",\n",
    "    \"        matches = re.findall(pattern, sentence.lower(), re.IGNORECASE)\\n\",\n",
    "    \"        current_matches.extend(matches)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Test enhanced patterns\\n\",\n",
    "    \"    enhanced_matches = []\\n\",\n",
    "    \"    for pattern in enhanced_anesthesia_patterns:\\n\",\n",
    "    \"        matches = re.findall(pattern, sentence.lower(), re.IGNORECASE)\\n\",\n",
    "    \"        enhanced_matches.extend(matches)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  Current patterns: {current_matches if current_matches else 'No matches'}\\\")\\n\",\n",
    "    \"    print(f\\\"  Enhanced patterns: {enhanced_matches if enhanced_matches else 'No matches'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Performance Metrics and Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Calculate performance metrics\\n\",\n",
    "    \"print(\\\"=== PERFORMANCE EVALUATION ===\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Processing time analysis\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"\\n\",\n",
    "    \"processing_times = []\\n\",\n",
    "    \"for report in sample_reports:\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Full pipeline\\n\",\n",
    "    \"    cleaned_text = preprocessor.clean_text(report['text'])\\n\",\n",
    "    \"    keyword_results = keyword_extractor.extract_all(cleaned_text)\\n\",\n",
    "    \"    entities = ner_extractor.extract_entities(cleaned_text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    end_time = time.time()\\n\",\n",
    "    \"    processing_times.append(end_time - start_time)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Average processing time per report: {np.mean(processing_times):.3f} seconds\\\")\\n\",\n",
    "    \"print(f\\\"Total processing time for {len(sample_reports)} reports: {sum(processing_times):.3f} seconds\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Coverage analysis\\n\",\n",
    "    \"print(\\\"COVERAGE ANALYSIS:\\\")\\n\",\n",
    "    \"total_extractions = 0\\n\",\n",
    "    \"possible_extractions = len(results_df) * len(categories)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for category in categories:\\n\",\n",
    "    \"    extracted = results_df[category].notna().sum()\\n\",\n",
    "    \"    total_extractions += extracted\\n\",\n",
    "    \"    print(f\\\"{category}: {extracted}/{len(results_df)} ({extracted/len(results_df)*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"overall_coverage = (total_extractions / possible_extractions) * 100\\n\",\n",
    "    \"print(f\\\"\\\\nOverall extraction coverage: {overall_coverage:.1f}%\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Quality assessment (manual review needed)\\n\",\n",
    "    \"print(\\\"QUALITY ASSESSMENT (Sample Review):\\\")\\n\",\n",
    "    \"for i, (_, row) in enumerate(results_df.head(3).iterrows()):\\n\",\n",
    "    \"    print(f\\\"\\\\nReport {row['report_id']}:\\\")\\n\",\n",
    "    \"    for category in categories:\\n\",\n",
    "    \"        value = row[category]\\n\",\n",
    "    \"        if pd.notna(value):\\n\",\n",
    "    \"            print(f\\\"  {category}: {value} ‚úì\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"  {category}: Not found\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Export and Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Export results for further analysis\\n\",\n",
    "    \"results_df.to_csv('../output/exploration_results.csv', index=False)\\n\",\n",
    "    \"print(\\\"‚úÖ Results exported to '../output/exploration_results.csv'\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Summary statistics\\n\",\n",
    "    \"print(\\\"\\\\n=== FINAL SUMMARY ===\\\")\\n\",\n",
    "    \"print(f\\\"üìä Processed {len(sample_reports)} stroke reports\\\")\\n\",\n",
    "    \"print(f\\\"‚ö° Average processing time: {np.mean(processing_times):.3f}s per report\\\")\\n\",\n",
    "    \"print(f\\\"üéØ Overall extraction coverage: {overall_coverage:.1f}%\\\")\\n\",\n",
    "    \"print(f\\\"üìù Total unique extractions: {total_extractions}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüèÜ BEST PERFORMING CATEGORIES:\\\")\\n\",\n",
    "    \"category_performance = [(cat, (results_df[cat].notna().sum() / len(results_df)) * 100) for cat in categories]\\n\",\n",
    "    \"category_performance.sort(key=lambda x: x[1], reverse=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for cat, perf in category_performance:\\n\",\n",
    "    \"    print(f\\\"  {cat.replace('_', ' ').title()}: {perf:.1f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüí° RECOMMENDATIONS:\\\")\\n\",\n",
    "    \"print(\\\"1. Expand pattern libraries for low-performing categories\\\")\\n\",\n",
    "    \"print(\\\"2. Add fuzzy matching for common medical abbreviations\\\")\\n\",\n",
    "    \"print(\\\"3. Consider context-aware extraction for ambiguous terms\\\")\\n\",\n",
    "    \"print(\\\"4. Implement confidence scoring for manual review prioritization\\\")\\n\",\n",
    "    \"print(\\\"5. Add support for multi-language medical terms (English/German mix)\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
